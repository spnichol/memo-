google <- read.csv("google_correlate.csv", header=T)
google(names)
names(google)
g.quant() <- google[c(3, 7, 4, 5)]
g.quant <- google[c(3, 7, 4, 5)]
(g.quant)
g.quant <- google[c(3, 7, 4, 5)]
cor(g.quant)
cor.test(g.quant$data_viz, g.quant$degree)
install.packages("Hmisc")
library("Hmisc")
rcorr(as.matrix(g.quant))
reg1 <- lm(data_viz ~
degree + stats_ed + facebook + nba + has_nba + region,
data = google)
summary(reg1)
names(sn)
names(sn)
sn <- read.csv("social_network.csv", header = T)
names(sn)
sn.tab <- table(sn$Gender, sn$Site)
sn.tab
margin.table(sn.tab, 1) # Row marginal frequencies
margin.table(sn.tab, 2) # Column marginal frequencies
round(prop.table(sn.tab), 2)    # cell %
round(prop.table(sn.tab, 1), 2) # row %
round(prop.table(sn.tab, 2), 2) # column %
chisq.test(sn.tab)
("google_correlate.csv", header = T)
names(google)
t.test(google$nba ~ google$has_nba)
names(google)
anova1 <- aov(data_viz ~ region, data = google)
summary(anova1)
anova2a <- aov(data_viz ~
region + stats_ed + region:stats_ed,
data = google)
summary(anova2a)
anova2b <- aov(data_viz ~
region*stats_ed,
data = google)
summary(anova2b)
vector<-c(5,9,1,0)
str(vector)
test <- "test1"
str(test)
getwd()
setwd("C:\Users\Steven\Google Drive\MOT\Fall 2016\BA\Week 1")   #using working directory
setwd("C:\\Users\\Steven\\Google Drive\\MOT\\Fall 2016\\BA\\Week 1")
data<- read.csv("\\Week-01\\zagat.csv", header=TRUE,stringsAsFactors=FALSE)
data<- read.csv("Week-01\\zagat.csv", header=TRUE,stringsAsFactors=FALSE)
names(data)
price<-data$Price
head(data)
str(data)
data$Price[1:10]
mean(data$Price)
quantile(data$Price)
cor(data$Price,data$Food)
summary(data$Price)
zagat<-data
service.sd <- sd(zagat$Service)
service.mean <- mean(zagat$Service)
z <- (zagat$Service-service.mean)/service.sd
zagat.z3 <- subset(zagat, z<3)
zagat.z2 <- subset(zagat, z<2)
zagat.z1 <- subset(zagat, z<1)
dim(zagat)
summary(zagat$Service)
esd(zagat$Service)
dim(zagat)
dim(zagat.z3)
summary(zagat.z3$Service)
sd(zagat.z3$Service)
z <- (zagat$Service-service.mean)/service.sd
zagat.z3 <- subset(zagat, z<3)
zagat.z2 <- subset(zagat, z<2)
zagat.z1 <- subset(zagat, z<1)
# Computing same calculation for data without outliers
dim(zagat)
summary(zagat$Service)
esd(zagat$Service)
dim(zagat.z3)
summary(zagat.z3$Service)
sd(zagat.z3$Service)
dim(zagat.z2)
summary(zagat.z2$Service)
sd(zagat.z2$Service)
dim(zagat.z1)
summary(zagat.z1$Service)
sd(zagat.z1$Service)
sd(zagat$Service)
par(mfrow=c(2,2))
hist(data$Food)
hist(data$Decor, col="blue")
hist(data$Service, col="green")
hist(data$Price, col="red")
install.package("quanteda")
install.library("quanteda")
install.packages("quanteda")
install.packages("tm"
mytext <- read.csv("NewsText.csv")
install.packages("tm")
mytext <- read.csv("NewsText.csv")
mytext <- read.csv("NewsText.csv")
library(quanteda)
library(stm)
library(tm)
library(NLP)
library(openNLP)
url<-
precorpus<- read.csv("~/Google Drive/GData/NewsText.csv",
header=TRUE, stringsAsFactors=FALSE)
url<-
precorpus<- read.csv("~/Google Drive/GData/NewsText.csv",
header=TRUE, stringsAsFactors=FALSE)
url<-
precorpus<- read.csv("~/Google Drive/GData/NewsText.csv",
header=TRUE, stringsAsFactors=FALSE)
header=TRUE, stringsAsFactors=FALSE)
url<-
precorpus<- read.csv("NewsText.csv",
header=TRUE, stringsAsFactors=FALSE)
dim(precorpus) # dim of file; 107 files with 11 headers
names(precorpus)   # names of the headers
head(precorpus)
str(precorpus)
require(quanteda)
help(corpus)
newscorpus<- corpus(precorpus$Full.text,
docnames=precorpus$Document_ID,
docvar=data.frame(Year=precorpus$Publication.year,Subject= precorpus$Subject))
newscorpus<- toLower(newscorpus, keepAcronyms = FALSE)
library(openNLP)
install.packages("openNLP")
library(openNLP)
help(corpus)
names(newscorpus)   #to explore the output of the corpus function: "documents" "metadata"  "settings"  "tokens"
summary(newscorpus)  #summary of corpus
newscorpus<- corpus(precorpus$Full.text,
docnames=precorpus$Document_ID,
docvar=data.frame(Year=precorpus$Publication.year,Subject= precorpus$Subject))
names(newscorpus)   #to explore the output of the corpus function: "documents" "metadata"  "settings"  "tokens"
summary(newscorpus)  #summary of corpus
newscorpus<- corpus(precorpus$Full.text,
docnames=precorpus$Author,
docvar=data.frame(Year=precorpus$Publication.year,Subject= precorpus$Subject))
names(newscorpus)   #to explore the output of the corpus function: "documents" "metadata"  "settings"  "tokens"
summary(newscorpus)  #summary of corpus
head(newscorpus)
newscorpus<- corpus(precorpus$Full.text,
docnames=precorpus$Author,
docvar=data.frame(Year=precorpus$Publication.year,Subject= precorpus$Subject))
#explore the corpus
names(precorpus)   # names of the headers
newscorpus<- corpus(precorpus$Full.text,
docnames=precorpus$Author,
docvar=data.frame(Year=precorpus$Publication.year,Subject= precorpus$Subject))
newscorpus<- corpus(precorpus$Full.text,
docnames=precorpus$Document_ID,
docvar=data.frame(Year=precorpus$Publication.year,Subject= precorpus$Subject))
newscorpus<- toLower(newscorpus, keepAcronyms = FALSE)
cleancorpus <- tokenize(newscorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE,
verbose=TRUE)
head(cleancorpus)    # text into token form
dfm.simple<- dfm(cleancorpus,
toLower = TRUE,
ignoredFeatures =stopwords("english"),
verbose=TRUE,
stem=FALSE)
head(dfm.simple) #explore output of dfm
topfeatures<-topfeatures(dfm.simple, n=50)
topfeatures
swlist = c("said", "mr", "can")
dfm.stem<- dfm(cleancorpus, toLower = TRUE,
ignoredFeatures = c(swlist, stopwords("english")),
verbose=TRUE,
stem=TRUE)
topfeatures.stem<-topfeatures(dfm.stem, n=50)
topfeatures.stem
dfm.stem<- dfm(cleancorpus, toLower = TRUE,
ignoredFeatures = c(swlist, stopwords("english")),
verbose=TRUE,
stem=FALSE)
topfeatures.stem<-topfeatures(dfm.stem, n=50)
topfeatures.stem
kwic(cleancorpus, "credit", 2) #2words before 2 words after
kwic(cleancorpus , "data analytics", window = 3)
cleancorpus <- tokenize(newscorpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE,
ngrams=2, verbose=TRUE)
dfm.bigram<- dfm(cleancorpus, toLower = TRUE,
ignoredFeatures = c(swlist, stopwords("english")),
verbose=TRUE,
stem=FALSE)
topfeatures.bigram<-topfeatures(dfm.bigram, n=50)
topfeatures.bigram
mydict <- dictionary(list(negative = c("detriment*", "bad*", "awful*", "terrib*", "horribl*"),
postive = c("good", "great", "super*", "excellent", "yay"))
dfm.sentiment <- dfm(cleancorpus, dictionary = mydict)
mydict <- dictionary(list(negative = c("detriment*", "bad*", "awful*", "terrib*", "horribl*"),postive = c("good", "great", "super*", "excellent", "yay")))
dfm.sentiment <- dfm(cleancorpus, dictionary = mydict)
topfeatures(dfm.sentiment)
View(dfm.sentiment)
library(wordcloud)
install.packages("wordcloud")
")
install.packages("tm")
")
install.packages("wordcloud")
install.packages("wordcloud")
install.packages("wordcloud")
library(wordcloud)
set.seed(142)   #keeps cloud' shape fixed
dark2 <- brewer.pal(8, "Set1")
freq<-topfeatures(dfm.stem, n=500)
wordcloud(names(freq),
freq, max.words=200,
scale=c(3, .1),
colors=brewer.pal(8, "Set1"))
dfm.tm<-convert(dfm.stem, to="tm")
findAssocs(dfm.tm,
c("data", "tech", "big"),
corlimit=0.6)
findAssocs(dfm.tm,
c("public","model", "create" ),
corlimit=0.7)
wordcloud(names(freq),
freq, max.words=200,
scale=c(3, .1),
colors=brewer.pal(8, "Set1"))
dfm.tm<-convert(dfm.stem, to="tm")
findAssocs(dfm.tm,
c("data", "tech", "big"),
corlimit=0.6)
findAssocs(dfm.tm,
c("public","model", "create" ),
corlimit=0.7)
dfm.tm<-convert(dfm.stem, to="tm")
findAssocs(dfm.tm,
c("data", "tech", "big"),
corlimit=0.6)
library(tm)
dfm.tm<-convert(dfm.stem, to="tm")
findAssocs(dfm.tm,
c("data", "tech", "big"),
corlimit=0.6)
help(findAssocs)
library(stm)
install.packages("stm")
library(stm)
temp<-textProcessor(documents=precorpus$Full.text, metadata = precorpus)
names(temp)  # produces:  "documents", "vocab", "meta", "docs.removed"
vocab<-temp$vocab
docs<-temp$documents
out <- prepDocuments(docs, vocab, meta)
docs<-out$documents
vocab<-out$vocab
meta <-out$meta
prevfit <-stm(docs , vocab ,
K=20,
verbose=TRUE,
data=meta,
max.em.its=25)
topics <-labelTopics(prevfit , topics=c(1:20))
topics   #shows topics with highest probability words
findThoughts(prevfit, texts = precorpus$Full.text,  topics = 10,  n = 2)
plot.STM(prevfit, type="summary")
plot.STM(prevfit, type="labels", topics=c(15,4,5))
plot.STM(prevfit, type="perspectives", topics = c(19,10))
mod.out.corr <- topicCorr(prevfit)  #Estimates a graph of topic correlations
plot.topicCorr(mod.out.corr)
install.packages("igraph")
mod.out.corr <- topicCorr(prevfit)  #Estimates a graph of topic correlations
plot.topicCorr(mod.out.corr)
plot.STM(prevfit, type="summary")
plot.STM(prevfit, type="labels", topics=c(15,4,5))
plot.STM(prevfit, type="perspectives", topics = c(19,10))
plot.topicCorr(mod.out.corr)
library(quanteda)
library(stm)
library(tm)
library(NLP)
library(openNLP)
library(ggplot2)
library(ggdendro)
library(cluster)
library(fpc)
install.packages("fpc")
library(fpc)
library(cluster)
library(ggdendro)
install.packages("ggendro")
install.packages("ggdendro")
library(cluster)
library(ggdendro)
require(quanteda)
summary(inaugCorpus)
corpus<- toLower(inaugCorpus, keepAcronyms = FALSE)
cleancorpus <- tokenize(corpus,
removeNumbers=TRUE,
removePunct = TRUE,
removeSeparators=TRUE,
removeTwitter=FALSE,
verbose=TRUE)
dfm<- dfm(cleancorpus,
toLower = TRUE,
ignoredFeatures =stopwords("english"),
verbose=TRUE,
stem=TRUE)
# Reviewing top features
topfeatures(dfm, 50)m     # displays 50 features
topfeatures(dfm, 50)   # displays 50 features
dfm.tm<-convert(dfm, to="tm")
dfm.tm
dtmss <- removeSparseTerms(dfm.tm, 0.15)
dtmss
d.dfm <- dist(t(dtmss), method="euclidian")
fit <- hclust(d=d.dfm, method="average")
hcd <- as.dendrogram(fit)
require(cluster)
k<-5
plot(hcd, ylab = "Distance", horiz = FALSE,
main = "Five Cluster Dendrogram",
edgePar = list(col = 2:3, lwd = 2:2))
rect.hclust(fit, k=k, border=1:5) # draw dendogram with red borders around the 5 clusters
dfm.tm
dtmss
ggdendrogram(fit, rotate = TRUE, size = 4, theme_dendro = FALSE,  color = "blue") +
xlab("Features") +
ggtitle("Cluster Dendrogram")
require(fpc)
d <- dist(t(dtmss), method="euclidian")
kfit <- kmeans(d, 5)
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0)
library(dplyr)
require(magrittr)
library(tm)
library(ggplot2)
library(stringr)
library(NLP)
library(openNLP)
url<-"https://raw.githubusercontent.com/jcbonilla/BusinessAnalytics/master/Week-08_and_09/NewsText.csv"
precorpus<- read.csv(url,
header=TRUE, stringsAsFactors=FALSE)
install.packages("dplyr")
news_2015<-precorpus$Full.text
stop_words <- stopwords("SMART")
stop_words <- c(stop_words, "said", "the", "also", "say", "just", "like","for",
"us", "can", "may", "now", "year", "according", "mr")
stop_words <- tolower(stop_words)
news_2015 <- news_2015[news_2015 != ""]
doc.list <- strsplit(news_2015, "[[:space:]]+")
term.table <- table(unlist(doc.list))
term.table <- sort(term.table, decreasing = TRUE)
del <- names(term.table) %in% stop_words | term.table < 5
term.table <- term.table[!del]
term.table <- term.table[names(term.table) != ""]
vocab <- names(term.table)
get.terms <- function(x) {
index <- match(x, vocab)
index <- index[!is.na(index)]
rbind(as.integer(index - 1), as.integer(rep(1, length(index))))
}
documents <- lapply(doc.list, get.terms)
D <- length(documents)  # number of documents (1)
W <- length(vocab)  # number of terms in the vocab (1741)
doc.length <- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]
N <- sum(doc.length)  # total number of tokens in the data (56196)
term.frequency <- as.integer(term.table)
K <- 10
G <- 3000
alpha <- 0.02
eta <- 0.02
library(lda)
set.seed(357)
t1 <- Sys.time()
fit <- lda.collapsed.gibbs.sampler(documents = documents, K = K, vocab = vocab,
num.iterations = G, alpha = alpha,
eta = eta, initial = NULL, burnin = 0,
compute.log.likelihood = TRUE)
t2 <- Sys.time()
t2 - t1
theta <- t(apply(fit$document_sums + alpha, 2, function(x) x/sum(x)))
phi <- t(apply(t(fit$topics) + eta, 2, function(x) x/sum(x)))
news_for_LDA <- list(phi = phi,
theta = theta,
doc.length = doc.length,
vocab = vocab,
term.frequency = term.frequency)
library(LDAvis)
install.packages("LDAvis")
library(servr)
instlal.packages("servr")
install.packages("servr")
library(LDAvis)
library(servr)
json <- createJSON(phi = news_for_LDA$phi,
theta = news_for_LDA$theta,
doc.length = news_for_LDA$doc.length,
vocab = news_for_LDA$vocab,
term.frequency = news_for_LDA$term.frequency)
serVis(json, out.dir = 'vis', open.browser = TRUE)
